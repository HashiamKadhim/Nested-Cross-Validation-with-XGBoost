{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested Cross Validation with Grid Search\n",
    "\n",
    "In order to impliment feature selection, hyperparameter tuning, and model selection, one could split the data into a training set and a test set, and apply cross validation on the training set. However this method uses the same data to tune model parameters and evaluate model performance which yields an overly-optimistic score because information may “leak” into the model and overfit the data. We refer to http://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html for an example. \n",
    "\n",
    "Hence if one where to impliment feature/hyperparameter/model selection prior to cross-validation, and the best combination of feature/hyperparameters for the best model is chosen to be the estimator, we end up with a biased estimator. To obtain an unbiased estimator, one could impliment feature selection, hyperparameter tuning, and model selection independently in each fold of the cross-validation. This is the concept of nested cross validation. Below is an excerpt from \"Temporal Video Segmentation\" by Christian Petersohn which provides pseudo-code for the algorithm. \n",
    "\n",
    "<img src=\"figures/nested_cross_validation.png\">\n",
    "\n",
    "If we just think about hyperparameter tuning for the moment, one could apply grid search (http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) and use the nested Scikit-Learn cross validation method by referring to http://scikit-learn.org/stable/tutorial/statistical_inference/model_selection.html. However there is a few drawbacks with doing it this way:\n",
    "\n",
    "* When using this method, you cannot nest objects with parallel computing\n",
    "* Even if I try to use parallel computing on the inner sklearn.model_selection.GridSearchCV method (that is $n_jobs \\neq 1$, It creates a copy of the data for each job. One way to combat this is by using the pre_dispatch parameter, but still, on my personal system, it will not run.\n",
    "* There is no implimentation of early stopping with the GridSearchCV method (for tools such as xgboost). I was able to find this hack (https://www.kaggle.com/c/liberty-mutual-group-property-inspection-prediction/forums/t/15235/xgboost-sklearn-api-gridsearch-early-stopping?forumMessageId=85447#post85447) but there were complaints that it didn't support stratified K-folds. Using stratified sampling is important for our problem because the target feature is imbalanced. \n",
    "\n",
    "For these reasons, I implemented my own nested cross validation with grid search, and the ability to use parallel computing, as well as being able to implement early stopping for xgboost. This can be seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside inner loop\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "Finished Inner Fold For Some Inner Model 1\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "Finished Inner Fold For Some Inner Model 2\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "Finished Inner Fold For Some Inner Model 3\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "Finished Inner Fold For Some Inner Model 4\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "Finished Inner Fold For Some Inner Model 5\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "Finished Inner Fold For Some Inner Model 6\n",
      "Finished Outer Fold 1 Score: 0.980491\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "Finished Inner Fold For Some Inner Model 1\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "Finished Inner Fold For Some Inner Model 2\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "Finished Inner Fold For Some Inner Model 3\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "Finished Inner Fold For Some Inner Model 4\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "Finished Inner Fold For Some Inner Model 5\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "Finished Inner Fold For Some Inner Model 6\n",
      "Finished Outer Fold 2 Score: 0.971714\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "Finished Inner Fold For Some Inner Model 1\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "Finished Inner Fold For Some Inner Model 2\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "Finished Inner Fold For Some Inner Model 3\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "Finished Inner Fold For Some Inner Model 4\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "Finished Inner Fold For Some Inner Model 5\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "Finished Inner Fold For Some Inner Model 6\n",
      "Finished Outer Fold 3 Score: 0.990854\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "Finished Inner Fold For Some Inner Model 1\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "Finished Inner Fold For Some Inner Model 2\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "Finished Inner Fold For Some Inner Model 3\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "Finished Inner Fold For Some Inner Model 4\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "Finished Inner Fold For Some Inner Model 5\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "inside inner loop\n",
      "Finished Inner Fold For Some Inner Model 6\n",
      "Finished Outer Fold 4 Score: 0.985195\n",
      "function took 0.13294274807 mins\n",
      "Estimated ROC_AUC: 0.9820635\n",
      "0 model max_depth: 14\n",
      "0 model min_child_weight: 13\n",
      "1 model max_depth: 14\n",
      "1 model min_child_weight: 13\n",
      "2 model max_depth: 14\n",
      "2 model min_child_weight: 13\n",
      "3 model max_depth: 14\n",
      "3 model min_child_weight: 13\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import itertools\n",
    "from sklearn import model_selection,metrics \n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost\n",
    "import time\n",
    "import sklearn.datasets\n",
    "\n",
    "X,Y = sklearn.datasets.make_classification(n_samples=1000)\n",
    "\n",
    "# data is split in a stratified fashion into train and test sets\n",
    "seed = 7\n",
    "test_size = 0.30\n",
    "X_train_final, X_test_final, y_train_final, y_test_final = model_selection.train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "\n",
    "\n",
    "\n",
    "outerK = 4\n",
    "innerK = 2\n",
    "early_stopping_rounds = 100\n",
    "eval_metric = 'auc'\n",
    "\n",
    "skf = StratifiedKFold(n_splits=outerK)\n",
    "skf.get_n_splits(X_train_final, y_train_final)\n",
    "\n",
    "parameters = {'nthread' : [-1],\n",
    "              'objective':['binary:logistic'],\n",
    "              'learning_rate': [0.1], #so called `eta` value\n",
    "              'max_depth': [6,14],\n",
    "              'min_child_weight': [1,7,13],\n",
    "              'silent': [1],\n",
    "              'n_estimators': [1000000], #number of trees, change it to 1000 for better results\n",
    "              'seed': [1337]}\n",
    "\n",
    "\n",
    "\n",
    "def nested_CV_GS_XGB(parameters):\n",
    "    \n",
    "    def my_product(dicts):\n",
    "        product = [x for x in apply(itertools.product, dicts.values())]\n",
    "        return [dict(zip(dicts.keys(), p)) for p in product]\n",
    "        \n",
    "    bestModels = []\n",
    "    bestModelScores = []\n",
    "    bestModelsOuterparams = []\n",
    "    topModels = []\n",
    "    outerCounter = 1\n",
    "    for train_index, test_index in skf.split(X_train_final, y_train_final):\n",
    "        X_train, X_test = X_train_final[train_index], X_train_final[test_index]\n",
    "        y_train, y_test = y_train_final[train_index], y_train_final[test_index]\n",
    "        \n",
    "        skfInner = StratifiedKFold(n_splits=innerK)\n",
    "        skf.get_n_splits(X_train, y_train)\n",
    "\n",
    "        tempModels = []\n",
    "        scores = []\n",
    "        innerCounter = 1\n",
    "        for params in my_product(parameters):\n",
    "            model = xgboost.XGBClassifier(**params)\n",
    "            for train_index_inner, test_index_inner in skf.split(X_train, y_train):\n",
    "                print 'inside inner loop'\n",
    "                X_train_inner, X_test_inner = X_train[train_index_inner], X_train[test_index_inner]\n",
    "                y_train_inner, y_test_inner = y_train[train_index_inner], y_train[test_index_inner]\n",
    "                model.fit(X_train_inner, y_train_inner,verbose = False, early_stopping_rounds=early_stopping_rounds, eval_metric=eval_metric, eval_set=[(X_test_inner, y_test_inner)])\n",
    "                scores.append(model.best_score)   \n",
    "            avgScore = float(sum(scores))/len(scores)\n",
    "            tempModels.append([model.get_params(), avgScore])\n",
    "            \n",
    "            print 'Finished Inner Fold For Some Inner Model', innerCounter\n",
    "            innerCounter+=1\n",
    "        tempModels.sort(key=lambda x: int(x[1]))\n",
    "        bestMod = tempModels[-1][0]\n",
    "        bestModels.append(bestMod)\n",
    "        \n",
    "\n",
    "        \n",
    "        outerModel = xgboost.XGBClassifier(**bestMod)\n",
    "        outerModel.fit(X_train, y_train,verbose = False, early_stopping_rounds=early_stopping_rounds, eval_metric=eval_metric,\n",
    "              eval_set=[(X_test, y_test)])\n",
    "\n",
    "        bestModelsOuterparams.append([[bestMod], [outerModel.best_score]])\n",
    "        bestModelScores.append(outerModel.best_score)\n",
    "        topModels.append(outerModel)\n",
    "        print 'Finished Outer Fold', outerCounter, 'Score:', outerModel.best_score\n",
    "        outerCounter+=1\n",
    "    avgBestModelScores = float(sum(bestModelScores))/len(bestModelScores)\n",
    "    \n",
    "    return avgBestModelScores,topModels,  bestModelsOuterparams\n",
    "\n",
    "\n",
    "t0 = time.time()\n",
    "roc_auc, topModels, bestModels = nested_CV_GS_XGB(parameters)\n",
    "t1 = time.time()\n",
    "print 'function took', float(t1-t0)/60, 'mins' \n",
    "print 'Estimated ROC_AUC:', roc_auc\n",
    "\n",
    "           \n",
    "for i in range(outerK):\n",
    "    print i, 'model max_depth:', bestModels[i][0][0]['max_depth']\n",
    "    print i, 'model min_child_weight:', bestModels[i][0][0]['min_child_weight']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each fold, it looks like the parameters max_depth = 14 and min_child_weight = 13 produced the best results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
